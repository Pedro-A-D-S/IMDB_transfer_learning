{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from transformers import TFBertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "INFO:absl:Load dataset info from /home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Reusing dataset imdb_reviews (/home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from /home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
      "2023-10-31 14:06:50.510175: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-31 14:06:50.510385: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-31 14:06:50.510397: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-31 14:06:50.510415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2023-10-31 14:06:50.512281: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "INFO:absl:Load dataset info from /home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
      "INFO:absl:Reusing dataset imdb_reviews (/home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split test, from /home/pop/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
     ]
    }
   ],
   "source": [
    "imdb_train, ds_info = tfds.load(\n",
    "    name='imdb_reviews',\n",
    "    split='train',\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "imdb_test = tfds.load(\n",
    "    name='imdb_reviews',\n",
    "    split='test',\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:06:50.663394: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-10-31 14:06:50.663966: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3393180000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Check and example from the dataset\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary and encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set,\n",
    "                                                   lowercase=True,\n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "this was an absolutely terrible movie don t be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie s ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor s like christopher walken s good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = imdb_train.map(\n",
    "    encode_tf_fn,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "encoded_test = imdb_test.map(\n",
    "    encode_tf_fn,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_w2v = {}\n",
    "\n",
    "with open('../data/raw/glove.6B.50d.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = np.array(tokens[1:], dtype=np.float32)\n",
    "        \n",
    "        if vector.shape[0] == 50:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            print('There was an issue with ' + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have a dataset, its vocabulary, and a dictionary of GloVe words and\n",
    "their corresponding vectors. However, there is no correlation between these two\n",
    "vocabularies. The way to connect them is through the creation of an embedding\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this embedding matrix of zeros is initialized, it needs to be populated. For each word in the vocabulary of reviews, the corresponding vector is retrieved from the\n",
    "GloVe dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_cnt = 0\n",
    "unk_set = set()\n",
    "\n",
    "for word in imdb_encoder.tokens:\n",
    "    embedding_vector = dict_w2v.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        tkn_id = imdb_encoder.encode(word)[0]\n",
    "        embedding_matrix[tkn_id] = embedding_vector\n",
    "    else:\n",
    "        unk_cnt += 1\n",
    "        unk_set.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data loading step, we saw that the total number of tokens was 93,931.\n",
    "Out of these, 14,553 words could not be found, which is approximately 15% of\n",
    "the tokens. For these words, the embedding matrix will have zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the **feature extraction** model freezes the pre-trained\n",
    "weights and does not update them. An important issue with this approach in the\n",
    "current setup is that there are a large number of tokens, over 14,000, that have\n",
    "zero embedding vectors. These words could not be matched to an entry in the\n",
    "GloVe word list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove based BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "rnn_units = 64\n",
    "\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm(\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int,\n",
    "    rnn_units: int,\n",
    "    batch_size: int,\n",
    "    train_emb: bool = False\n",
    ") -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_dim,\n",
    "                mask_zero=True,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=train_emb,\n",
    "            ),\n",
    "            Dropout(0.25),\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.5)\n",
    "            ),\n",
    "            Bidirectional(LSTM(\n",
    "                rnn_units,\n",
    "                dropout=0.25)),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          4696550   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         58880     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 157,825\n",
      "Non-trainable params: 4,696,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fe = build_model_bilstm(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model_fe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has about 4.8 million trainable parameters. A simpler or smaller model will train faster and possibly be less\n",
    "likely to overfit as the model capacity is lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fe.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', 'Precision', 'Recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb3379dc70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.fit(encoded_train_batched, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 32s 112ms/step - loss: 0.3878 - accuracy: 0.8336 - precision: 0.7792 - recall: 0.9309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38784679770469666,\n",
       " 0.8335599899291992,\n",
       " 0.7792138457298279,\n",
       " 0.9308800101280212]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fe.save('../models/feature-extraction-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          4696550   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 128)         58880     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 4,854,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = build_model_bilstm(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_emb=True\n",
    ")\n",
    "\n",
    "model_ft.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is identical to the feature extraction model in size. However, since the\n",
    "embeddings will be fine-tuned, training is expected to take a little longer. There\n",
    "are several thousand zero embeddings, which can now be updated. The resulting\n",
    "accuracy is expected to be much better than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', 'Precision', 'Recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 72s 284ms/step - loss: 0.3898 - accuracy: 0.8209 - precision: 0.8192 - recall: 0.8235\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 72s 286ms/step - loss: 0.3854 - accuracy: 0.8245 - precision: 0.8251 - recall: 0.8235\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 72s 286ms/step - loss: 0.3845 - accuracy: 0.8224 - precision: 0.8220 - recall: 0.8230\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 72s 286ms/step - loss: 0.3850 - accuracy: 0.8214 - precision: 0.8213 - recall: 0.8215\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 72s 285ms/step - loss: 0.3768 - accuracy: 0.8288 - precision: 0.8305 - recall: 0.8262\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 73s 291ms/step - loss: 0.3772 - accuracy: 0.8284 - precision: 0.8291 - recall: 0.8272\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 73s 289ms/step - loss: 0.3710 - accuracy: 0.8330 - precision: 0.8327 - recall: 0.8334\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 73s 291ms/step - loss: 0.3708 - accuracy: 0.8316 - precision: 0.8302 - recall: 0.8336\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 73s 291ms/step - loss: 0.3677 - accuracy: 0.8322 - precision: 0.8324 - recall: 0.8319\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 76s 304ms/step - loss: 0.3670 - accuracy: 0.8364 - precision: 0.8347 - recall: 0.8388\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 77s 305ms/step - loss: 0.3608 - accuracy: 0.8359 - precision: 0.8331 - recall: 0.8401\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 77s 305ms/step - loss: 0.3556 - accuracy: 0.8346 - precision: 0.8332 - recall: 0.8366\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 76s 304ms/step - loss: 0.3582 - accuracy: 0.8400 - precision: 0.8398 - recall: 0.8402\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 77s 308ms/step - loss: 0.3522 - accuracy: 0.8419 - precision: 0.8413 - recall: 0.8429\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 82s 327ms/step - loss: 0.3539 - accuracy: 0.8402 - precision: 0.8395 - recall: 0.8413\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 0.3511 - accuracy: 0.8405 - precision: 0.8381 - recall: 0.8440\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 79s 314ms/step - loss: 0.3521 - accuracy: 0.8434 - precision: 0.8430 - recall: 0.8440\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 72s 287ms/step - loss: 0.3470 - accuracy: 0.8435 - precision: 0.8418 - recall: 0.8461\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 73s 291ms/step - loss: 0.3469 - accuracy: 0.8440 - precision: 0.8439 - recall: 0.8441\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 72s 288ms/step - loss: 0.3431 - accuracy: 0.8463 - precision: 0.8436 - recall: 0.8502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb04464af0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.fit(encoded_train_batched, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 35s 113ms/step - loss: 0.6953 - accuracy: 0.4968 - precision: 0.4971 - recall: 0.9244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6952202320098877,\n",
       " 0.49779999256134033,\n",
       " 0.49880772829055786,\n",
       " 0.9204000234603882]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.save('../models/fine-tuning-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Based Transfer Learning with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    bert_name,\n",
    "    add_special_tokens=True,\n",
    "    do_lower_case=False,\n",
    "    max_length=150,\n",
    "    pad_to_max_length=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encoder(review):\n",
    "    txt = review.numpy().decode('utf-8')\n",
    "    encoded = tokenizer.encode_plus(txt, add_special_tokens=True, \n",
    "                                    max_length=150, pad_to_max_length=True, \n",
    "                                    return_attention_mask=True, \n",
    "                                    return_token_type_ids=True,\n",
    "                                    truncation=True)\n",
    "    return encoded['input_ids'], encoded['token_type_ids'], \\\n",
    "           encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train = [bert_encoder(r) for r, l in imdb_train]\n",
    "bert_lbl = [l for r, l in imdb_train]\n",
    "bert_train = np.array(bert_train)\n",
    "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    bert_train,\n",
    "    bert_lbl,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_reviews, tr_segments, tr_masks = np.split(x_train, 3, axis=1)\n",
    "val_reviews, val_segments, val_masks = np.split(x_val, 3, axis=1)\n",
    "\n",
    "tr_reviews = tr_reviews.squeeze()\n",
    "tr_segments = tr_segments.squeeze()\n",
    "tr_masks = tr_masks.squeeze()\n",
    "\n",
    "val_reviews = val_reviews.squeeze()\n",
    "val_segments = val_segments.squeeze()\n",
    "val_masks = val_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((tr_reviews, tr_masks, \n",
    "                                               tr_segments, y_train)).\\\n",
    "            map(example_to_features).shuffle(100).batch(16)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, val_masks, \n",
    "                                               val_segments, y_val)).\\\n",
    "            map(example_to_features).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-built BERT classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 597kB/s]\n",
      "Downloading: 100%|██████████| 527M/527M [00:23<00:00, 22.3MB/s] \n",
      "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_tf_utils:Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_39', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "bert_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on IMDB\n",
      "Epoch 1/3\n",
      "   6/1250 [..............................] - ETA: 3:06:34 - loss: 0.7336 - accuracy: 0.4469"
     ]
    }
   ],
   "source": [
    "print('Fine-tuning BERT on IMDB')\n",
    "bert_history = bert_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=valid_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test = [bert_encoder(r) for r, l in imdb_test]\n",
    "bert_tst_lbl = [l for r, l in imdb_test]\n",
    "bert_test2 = np.array(bert_test)\n",
    "bert_tst_lbl2 = tf.keras.utils.to_categorical(bert_tst_lbl, num_classes=2)\n",
    "ts_reviews, ts_segments, ts_masks = np.split(bert_test2, 3, axis=1)\n",
    "ts_reviews = ts_reviews.squeeze()\n",
    "ts_segments = ts_segments.squeeze()\n",
    "ts_masks = ts_masks.squeeze()\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (ts_reviews, ts_masks, ts_segments, bert_tst_lbl2)\n",
    "    )\n",
    "    .map(example_to_features)\n",
    "    .shuffle(100)\n",
    "    .batch(16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 19:19:28.427635: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-07 19:19:28.444748: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-07 19:19:28.444766: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-07 19:19:28.444784: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2023-11-07 19:19:28.446847: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_name = 'bert-base-cased'\n",
    "bert = TFBertModel.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 150\n",
    "\n",
    "inp_ids = Input((max_seq_len), dtype = tf.int64, name='input_ids')\n",
    "att_mask = Input((max_seq_len), dtype = tf.int64, name='attention_mask')\n",
    "seq_ids = Input((max_seq_len), dtype = tf.int64, name='token_type_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dict = {\n",
    "    'inputs_ids': inp_ids,\n",
    "    'attention_mask': att_mask,\n",
    "    'token_type_ids': seq_ids\n",
    "}\n",
    "\n",
    "outputs = bert(inp_dict)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.2)(outputs[1])\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "custom_model = Model(inputs=inp_dict, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "custom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Custom Model: Fine-tuning BERT on IMDB')\n",
    "custom_history = custom_model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=valid_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.trainable = False\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "custom_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom Model: Fine-tuning BERT on IMDB\")\n",
    "custom_history = custom_model.fit(train_ds, epochs=2,\n",
    "                                  validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
